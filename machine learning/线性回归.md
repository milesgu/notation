####基本介绍

线性回归从几何的角度来解释，就是在(n+1)维的向量空间（n维特征+1维结果）找到一个n维的超平面，用这个超平面去拟合数据。

假设有n个特征，分别用$x_1, x_2, …, x_n$ 表示，用如下```hypothesis function```进行回归。

$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$

为了使形式统一，引入$x_0=1$，得

$h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n=\sum_{i=0}^n\theta_ix_i$ ,很容易写成向量形式



下面对记号进行说明

$x_j^{(i)}$ 第i个样本的第j个特征的取值

$x^{(i)}$ 第i个样本的特征的**向量**形式 （n+1维，注意前面引入了“伪特征”$x_0$）

$m$ 训练样本的个数

n 特征的个数



为了衡量回归（拟合）的效果，引入```cost function``` 如下

$J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$  

线性回归的cost function是一个**凸函数**，因此只有一个全局最小值



利用**梯度下降法（Gradient descent）**改变$\theta$的取值，以使得cost function值逐渐减小

Repeat{

​	$\theta_j:=\theta_j-\frac{\alpha}{m}\sum(h_\theta(x^{(i)})-y^{(i)}))x_j^{(i)}$ 

}

这个公式是针对每个$\theta$求导得到的

表示成向量形式为

Repeat{

​	$\theta=\theta-\frac{\alpha}{m} X^{T}(X\theta-y)$

}



为了检查是否已经取得最优值（使得cost function最小的$\theta$值），比较好的做法是将每次迭代的cost function值都记录下来，然后作图，观察cost function值的变化曲线，如果类似如下曲线，则说明已经收敛。甚至可以将$\alpha$ 略微调大，已增加收敛速度。

 ![TIM图片20180314173859](C:\Users\Milesgo\Desktop\笔记图库\TIM图片20180314173859.png)

如果没有出现平滑部分，这说明迭代次数不够，需要增加迭代次数

如果出现了千奇百怪，甚至递增的情况，这说明$\alpha$ 太大，需要调小一点

####特征归一化(feature scaling)

如果不归一化，各维特征的跨度差距很大，目标函数就会是“扁”的（这里是以二维为例）：

![899056-20180309162514760-1131169732](C:\Users\Milesgo\Desktop\笔记图库\899056-20180309162514760-1131169732.png)



（图中椭圆表示目标函数的等高线，两个坐标轴代表两个特征）
这样，在进行梯度下降的时候，梯度的方向就会偏离最小值的方向，走很多弯路。

如果归一化了，那么目标函数就“圆”了：

![899056-20180309162527427-1804447034](C:\Users\Milesgo\Desktop\笔记图库\899056-20180309162527427-1804447034.png)

**因为梯度方向垂直于等高线的切线，所以画一下图，两者的收敛速度对比就一目了然了**



##### 特征归一化的方法

对于每一维特征，求出最大值MAX和最小值MIN，求出平均值MEAN，则每个样本的该维特征值a应归一化为$\frac{a-MENA}{MAX-MIN}$ ，也可以求出方差STD并将a归一化为$\frac{a-MEAN}{STD}$ 。总之，无论归一化的方法是什么，最终归一化的目标是，每维的特征取值范围均接近[-1,1]，像[-3,0]这样的数量很接近的都是合理的。



#### $\alpha$ 的取值技巧

$\alpha$ 对是否能够收敛的影响前面已经讲过，一般取下列值

$…, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, …$



#### 多项式回归（Polynomial regression）

假设原来有两个特征x1，x2，则一种可能的多项式回归表达式如下

$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_1^2+\theta_3x_2+\theta_4x_2^2+\theta_5x_2^3$ 

多项式回归很容易转化为线性回归，例如上式可以转化为

$h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\theta_4x_4+\theta_5x_5$ 

其中$x_0=1, x_1=x_1,x_2=x_1^2, x_3=x_2, x_4=x_2^2, x_5=x_2^3$ 

当然在考虑多项式回归时，要顾及多项式的曲线形式，否则极有可能出现所有数据的拟合效果不错，但是预测在别的特征取值区间却是完全错误的。

当然也可以采用$h_\theta(x)=\theta_0+\theta_1x+\theta_2\sqrt x$ 等进行回归。



#### 直接计算法（对比梯度下降法）

由于线性回归的cost function是凸函数，所以必然存在唯一全局最小值，所以可以通过代数方法求出最优$\theta$ 

这里不进行推导，直接给出公式

$\theta=(X^TX)^{-1}X^Ty$ 

其中矩阵$A=X^TX$ 为n+1维的方阵，不一定是可逆方阵，但是这个公式还是成立的，因为有pseudo inverse的概念，这里也不展开，在MATLAB中，这个可以调用pinv进行求解。

什么情况可能造成矩阵A不可逆呢？样本的个数过少，或者各维特征线性相关。两者都可以通过删去某些特征进行解决。另外还可以使用**Regularization**的方法。

| 梯度下降法(Gradient Descent) | 代数方法(Normal Equation) |
| ---------------------------- | ------------------------- |
| 需要$\alpha$                 | 不需要$\alpha$            |
| 需要迭代                     | 不需要迭代                |
| 需要特征归一化               | 不需要特征归一化          |
| 时间复杂度$O(kn^2)$          | 时间复杂度$O(n^3)$        |

