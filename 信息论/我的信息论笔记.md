信息熵、交叉熵、相对熵、互信息、点互信息、条件互信息

引入随机变量X、Y

#### 信息熵

$H(X)=\sum_x p(x) log \frac{1}{p(x)}$ 

信息熵的值等同于平均最小编码量

即信息量越大的取值，编码位数越多

表示的是平均信息量

#### 交叉熵

$H(X)=\sum_x p(x)log\frac{1}{q(x)}$

当不知道真实分布$p(x)$的时候，可能会用另外的分布$q(x)$，此时的平均信息量就是交叉熵

交叉熵永远大于等于信息熵，当且仅当q(x)与p(x)同分布时，交叉熵等于信息熵

#### 相对熵

相对熵=交叉熵-信息熵

$H(X)=\sum_xp(x)log\frac{1}{q(x)}-\sum_xp(x)log\frac{1}{p(x)}=\sum_xp(x)log\frac{p(x)}{q(x)}$ 

#### 互信息

有两个随机变量X、Y

对于随机变量X，没有更多信息的时候X的信息熵为H(X)

当给出随机变量Y的时候，可能对随机变量X的不确定性造成影响，此时随机变量X的真实信息熵应为H(X|Y)

那么随机变量X、Y的互信息就定义为确定了随机变量Y后对随机变量X的不确定性的削弱程度

即$I[X,Y]=H(X)-H(X|Y)=H(Y)-H(Y|X)$

互信息刻画的是两个随机变量的相关程度

下面给出互信息经过推导后的公式

![20160617091445667](C:\Users\Milesgo\Desktop\笔记图库\20160617091445667.png)

#### 点互信息

根据上面推导的互信息的公式，可以得到点互信息的公式为

$I(x;y)=log_2\frac{p(x,y)}{p(x)p(y)}$ 

互信息实际上就是平均点互信息

#### 条件互信息

《机器学习》P155 提到了条件互信息的概念